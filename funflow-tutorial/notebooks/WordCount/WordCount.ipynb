{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: WordCount\n",
    "\n",
    "In this example, we'll implement a simple pipeline which \n",
    "calculates word frequencies in a plain text document. \n",
    "We'll use the smart constructors `pureFlow` and `ioFlow`, allowing\n",
    "definition of pipeline's tasks in terms of Haskell functions. \n",
    "\n",
    "This example may look familiar to users of Apache Beam, which \n",
    "also includes a WordCount example https://beam.apache.org/get-started/wordcount-example/.\n",
    "\n",
    "## Imports\n",
    "\n",
    "First, we'll need to import some additional modules which will enable us to \n",
    "more easily work with text (`Data.Text`), define dictionaries/maps (`Data.Map`), perform\n",
    "regex matching (`Text.Regex.Posix`), and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    ":opt no-lint\n",
    "\n",
    "{-# LANGUAGE Arrows #-}\n",
    "{-# LANGUAGE OverloadedStrings #-}\n",
    "\n",
    "import Data.List (sortBy)\n",
    "import Data.Map (Map)\n",
    "import qualified Data.Map as Map\n",
    "import Data.Ord (comparing)\n",
    "import qualified Data.Text.IO as T\n",
    "import qualified Data.Text as T\n",
    "import Text.Printf (printf)\n",
    "import Text.Regex.Posix ((=~))\n",
    "\n",
    "import Funflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Since we're opting to write our pipeline using function-based `Flow`s, we need to define the \n",
    "functions which will take care of parsing input text and counting words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- | (word, n_occurences)\n",
    "type TextCount = (T.Text, Int)\n",
    "\n",
    "-- | Counts members in a list of text. Also works for lazy lists (e.g. data from readFile)\n",
    "countWords :: [T.Text] -> [TextCount]\n",
    "countWords ws = let\n",
    "    tally :: T.Text -> Map T.Text Int -> Map T.Text Int\n",
    "    tally k = Map.insertWith (+) k 1\n",
    "    in \n",
    "        Map.toList $ foldr tally Map.empty ws\n",
    "\n",
    "-- | Removes punctuation marks from a text\n",
    "removePunctuation :: T.Text -> T.Text\n",
    "removePunctuation = \n",
    "    let\n",
    "        punctuation = \",.?!:;\\\"\\'\" :: String\n",
    "      in T.filter (not . (`elem` punctuation))\n",
    "\n",
    "\n",
    "-- | Filters words which are not comprised of latin characters (hyphens are allowed)\n",
    "filterWords :: [T.Text] -> [T.Text]\n",
    "filterWords = \n",
    "    let \n",
    "        wordsRegex = \"^[A-Za-z-]+$\" :: String\n",
    "      in filter $ (=~ wordsRegex) . T.unpack\n",
    "\n",
    "-- | Sorts a list of word counts in descending order\n",
    "sortCountsDesc :: [TextCount] -> [TextCount]\n",
    "sortCountsDesc = sortBy (flip $ comparing snd)\n",
    "\n",
    "-- | Like countWords, but with sorted results\n",
    "countWordsSortedDesc :: [T.Text] -> [TextCount]\n",
    "countWordsSortedDesc = sortCountsDesc . countWords\n",
    "\n",
    "-- | Prepare word counts for printing\n",
    "formatCounts :: [TextCount] -> [T.Text]\n",
    "formatCounts = map (\\(w,c) -> T.pack $ printf \"%s: %d\" (T.unpack w) c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Definition\n",
    "\n",
    "With our core utility functions defined, we're ready to define our pipeline. One \n",
    "simple way to structure our pipeline is to divide it into three tasks/operations:\n",
    "\n",
    "1. Read the input text file \n",
    "2. Parse the input text and return a summary of the word frequencies in it \n",
    "3. Write out our results. For this example, we can just write them directly to the terminal.\n",
    "\n",
    "Remember that in `funflow`, we create flows, each containing at least one task, and combine them \n",
    "into a final, larger `Flow` DAG. This is different than how DAGs are constructed in other workflow \n",
    "frameworks like Apache Airflow, where the _tasks_ are what get composed together. The advantage\n",
    "of making entire `Flow` DAGs composable is that you can re-use entire subsections of your workflow, \n",
    "instead of individual tasks. In other words, the recursive nature of `Flow` automatically scales reusability.\n",
    "\n",
    "For example, say that you write a `Flow` which contains some complex branching logic for reporting errors\n",
    "based on its input values (e.g. send a Slack message to the dev team if an upstream task reports an error). \n",
    "With `funflow`, you can simply re-use that error-reporting `Flow` across all of your various workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Individual task definitions (remember that each task is also a full \"Flow\")\n",
    "readDocument :: Flow String T.Text\n",
    "readDocument = ioFlow T.readFile\n",
    "\n",
    "countWordsAndSummarize :: Flow T.Text T.Text\n",
    "countWordsAndSummarize = pureFlow $ (T.unlines . formatCounts . countWordsSortedDesc . filterWords . T.words . removePunctuation)\n",
    "\n",
    "writeResult :: Flow (String, T.Text) ()\n",
    "writeResult = let\n",
    "        writeOutputMessage :: (String, T.Text) -> IO ()\n",
    "        writeOutputMessage (f, countText) = do\n",
    "            T.putStrLn \"Normally we would write the result to a file with T.writeFile, but for this example we can instead print the output:\"\n",
    "            T.putStrLn countText \n",
    "            return ()\n",
    "    in ioFlow writeOutputMessage\n",
    "\n",
    "-- Build the final pipeline using the task Flows defined above\n",
    "--   Note: Using arrow syntax to control which inputs get passed to\n",
    "--   which pipeline tasks, i.e. result_name <- task <- task_input\n",
    "flow :: Flow (String, String) ()\n",
    "flow = proc (documentFilePath, outputSummaryFilePath) -> do\n",
    "    documentText <- readDocument -< documentFilePath\n",
    "    countSummary <- countWordsAndSummarize -< documentText\n",
    "    writeResult -< (outputSummaryFilePath, countSummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "\n",
    "And finally, with our pipeline defined, we're ready to run it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normally we would write the result to a file with T.writeFile, but for this example we can instead print the output:\n",
       "a: 3\n",
       "and: 3\n",
       "it: 3\n",
       "try: 3\n",
       "words: 3\n",
       "Lets: 2\n",
       "This: 2\n",
       "count: 2\n",
       "give: 2\n",
       "or: 2\n",
       "pipeline: 2\n",
       "should: 2\n",
       "FILE: 1\n",
       "WordCounths: 1\n",
       "accept: 1\n",
       "basic: 1\n",
       "but: 1\n",
       "complex-words: 1\n",
       "contains: 1\n",
       "haskell: 1\n",
       "like: 1\n",
       "list: 1\n",
       "long: 1\n",
       "not: 1\n",
       "numbers: 1\n",
       "of: 1\n",
       "only: 1\n",
       "punctuation: 1\n",
       "the: 1\n",
       "to: 1\n",
       "using: 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runFlow flow (\"words.txt\":: String, \"outputs/counts.txt\"::String) :: IO ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell - haskell",
   "language": "haskell",
   "name": "ihaskell_haskell"
  },
  "language_info": {
   "codemirror_mode": "Haskell",
   "file_extension": ".hs",
   "mimetype": "text/x-haskell",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
